{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5e43b6fc-007f-4eb0-aa58-54a598bdf2db",
      "metadata": {
        "id": "5e43b6fc-007f-4eb0-aa58-54a598bdf2db"
      },
      "source": [
        "# b) Finding the best equation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "26caa802-f5ff-4a13-9fae-5ca6162a9e9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26caa802-f5ff-4a13-9fae-5ca6162a9e9f",
        "outputId": "575ec817-82c8-4f3c-de40-2325e33bafba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     material  Formula CrystalSystem   bulk  shear   young  poisson  Hexp\n",
            "0       mp-66  Diamond         cubic  435.3  520.5  1116.5     0.07  96.0\n",
            "1    mp-30148     BC2N  orthorhombic  361.0  422.7   912.1     0.08  76.0\n",
            "2   mp-629458     BC2N  orthorhombic  361.6  409.0   891.1     0.09  76.0\n",
            "3  mp-1018649    c-BC5      trigonal  405.8  378.2   865.6     0.14  71.0\n",
            "4     mp-1639       BN         cubic  408.0  374.5   860.2     0.15  63.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_csv(\"hardness_dataset.csv\")\n",
        "\n",
        "# Check the first few rows to understand the structure\n",
        "print(dataset.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0306d1f8-d081-472d-946f-2135d2f2a5fe",
      "metadata": {
        "id": "0306d1f8-d081-472d-946f-2135d2f2a5fe"
      },
      "outputs": [],
      "source": [
        "# Define the equations for predicting hardness\n",
        "def H1(G):\n",
        "    return 0.1475 * G\n",
        "\n",
        "def H2(Y):\n",
        "    return 0.0607 * Y\n",
        "\n",
        "def H3(G):\n",
        "    return 0.1769 * G - 2.899\n",
        "\n",
        "def H4(Y):\n",
        "    return 0.0635 * Y\n",
        "\n",
        "def H5(B, v):\n",
        "    return (1 - 2 * v) * B / (6 * (1 + v))\n",
        "\n",
        "def H6(G, B):\n",
        "    k = G / B\n",
        "    return 2*(((k**2) *G)**0.585 ) -3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "131e0126-b3f7-4ce5-8c47-2d402790e41b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "131e0126-b3f7-4ce5-8c47-2d402790e41b",
        "outputId": "485487fa-a062-4a20-fab5-96813a1ebfc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average error for each equation:\n",
            "H1_error    3.326114\n",
            "H2_error    3.655455\n",
            "H3_error    3.525926\n",
            "H4_error    3.861047\n",
            "H5_error    4.048435\n",
            "H6_error    4.081855\n",
            "dtype: float64\n",
            "Best equation on average: H1_error with an average error of 3.3261\n"
          ]
        }
      ],
      "source": [
        "# Calculate predicted hardness for each equation\n",
        "dataset['H1_pred'] = H1(dataset['shear'])\n",
        "dataset['H2_pred'] = H2(dataset['young'])\n",
        "dataset['H3_pred'] = H3(dataset['shear'])\n",
        "dataset['H4_pred'] = H4(dataset['young'])\n",
        "dataset['H5_pred'] = H5(dataset['bulk'], dataset['poisson'])\n",
        "dataset['H6_pred'] = H6(dataset['shear'], dataset['bulk'])\n",
        "\n",
        "# Calculate the absolute error for each equation\n",
        "dataset['H1_error'] = abs(dataset['Hexp'] - dataset['H1_pred'])\n",
        "dataset['H2_error'] = abs(dataset['Hexp'] - dataset['H2_pred'])\n",
        "dataset['H3_error'] = abs(dataset['Hexp'] - dataset['H3_pred'])\n",
        "dataset['H4_error'] = abs(dataset['Hexp'] - dataset['H4_pred'])\n",
        "dataset['H5_error'] = abs(dataset['Hexp'] - dataset['H5_pred'])\n",
        "dataset['H6_error'] = abs(dataset['Hexp'] - dataset['H6_pred'])\n",
        "\n",
        "# For each data point, find the equation with the minimum error\n",
        "dataset['best_equation'] = dataset[['H1_error', 'H2_error', 'H3_error', 'H4_error', 'H5_error', 'H6_error']].idxmin(axis=1)\n",
        "\n",
        "# Calculate the average error for each equation\n",
        "average_errors = dataset[['H1_error', 'H2_error', 'H3_error', 'H4_error', 'H5_error', 'H6_error']].mean()\n",
        "\n",
        "# Print the average error for each equation\n",
        "print(\"Average error for each equation:\")\n",
        "print(average_errors)\n",
        "\n",
        "# Find the best equation on average over the entire dataset\n",
        "best_equation_overall = average_errors.idxmin()\n",
        "best_equation_avg_error = average_errors.min()\n",
        "\n",
        "print(f\"Best equation on average: {best_equation_overall} with an average error of {best_equation_avg_error:.4f}\")\n",
        "\n",
        "# Optionally, save the dataset with the best equation for each data point\n",
        "#dataset.to_csv(\"hardness_with_best_equation.csv\", index=False)\n",
        "\n",
        "# Optionally, display the first few rows of the dataset with the best equations\n",
        "#print(dataset[['Hexp', 'H1_pred', 'H2_pred', 'H3_pred', 'H4_pred', 'H5_pred', 'H6_pred', 'best_equation']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be4bf173-b30e-40b7-8818-845bdf6854c1",
      "metadata": {
        "id": "be4bf173-b30e-40b7-8818-845bdf6854c1"
      },
      "source": [
        "# c) Classification using ANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e5c533f-dce6-446d-ad3d-850d9aeb4e6e",
      "metadata": {
        "id": "5e5c533f-dce6-446d-ad3d-850d9aeb4e6e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e6654cd5-aad2-4b05-ba75-107dee19091d",
      "metadata": {
        "id": "e6654cd5-aad2-4b05-ba75-107dee19091d"
      },
      "outputs": [],
      "source": [
        "#preprocessing\n",
        "\n",
        "#working on a copied dataset\n",
        "df = dataset.copy()\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def label_encoder(label):\n",
        "    le = LabelEncoder()\n",
        "    df[label] = le.fit_transform(df[label])\n",
        "\n",
        "label_list = [\"best_equation\"]\n",
        "for l in label_list:\n",
        "    label_encoder(l)\n",
        "\n",
        "# setup xdata, which is input and y data which is target\n",
        "xInput = df[['bulk', 'shear', 'young', 'poisson']]\n",
        "yTarget = df[\"best_equation\"]\n",
        "\n",
        "# split data for training and testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(xInput,yTarget, test_size=0.2, random_state=0)\n",
        "\n",
        "#scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "x_train = sc.fit_transform(x_train)\n",
        "x_test = sc.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6444668c-9e48-4925-8799-677d97f55227",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6444668c-9e48-4925-8799-677d97f55227",
        "outputId": "2fce1bae-0529-479b-b8f0-296364090874"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 136ms/step - accuracy: 0.1572 - loss: 9.2166 - val_accuracy: 0.2069 - val_loss: 10.2123\n",
            "Epoch 2/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1379 - loss: 8.7753 - val_accuracy: 0.2069 - val_loss: 10.2121\n",
            "Epoch 3/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1751 - loss: 10.0193 - val_accuracy: 0.2414 - val_loss: 10.2120\n",
            "Epoch 4/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1682 - loss: 9.9911 - val_accuracy: 0.2759 - val_loss: 10.2119\n",
            "Epoch 5/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1848 - loss: 10.4354 - val_accuracy: 0.2414 - val_loss: 10.2118\n",
            "Epoch 6/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2237 - loss: 10.0177 - val_accuracy: 0.2414 - val_loss: 10.2118\n",
            "Epoch 7/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2506 - loss: 9.6482 - val_accuracy: 0.2414 - val_loss: 10.2118\n",
            "Epoch 8/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2552 - loss: 9.9990 - val_accuracy: 0.2759 - val_loss: 10.2117\n",
            "Epoch 9/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1947 - loss: 10.3009 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 10/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1614 - loss: 9.3289 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 11/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2137 - loss: 11.2031 - val_accuracy: 0.2069 - val_loss: 10.2117\n",
            "Epoch 12/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2097 - loss: 10.1339 - val_accuracy: 0.2069 - val_loss: 10.2117\n",
            "Epoch 13/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2421 - loss: 10.5927 - val_accuracy: 0.2069 - val_loss: 10.2117\n",
            "Epoch 14/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1750 - loss: 9.1187 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 15/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2929 - loss: 9.7006 - val_accuracy: 0.2759 - val_loss: 10.2117\n",
            "Epoch 16/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2734 - loss: 10.5749 - val_accuracy: 0.2759 - val_loss: 10.2117\n",
            "Epoch 17/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1802 - loss: 10.0767 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 18/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2126 - loss: 8.9991 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 19/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1798 - loss: 9.7445 - val_accuracy: 0.2069 - val_loss: 10.2117\n",
            "Epoch 20/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2154 - loss: 10.1748 - val_accuracy: 0.2069 - val_loss: 10.2117\n",
            "Epoch 21/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1807 - loss: 9.0298 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 22/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1501 - loss: 10.4358 - val_accuracy: 0.2759 - val_loss: 10.2117\n",
            "Epoch 23/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1988 - loss: 10.6899 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 24/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2329 - loss: 10.1634 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 25/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1993 - loss: 9.6637 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 26/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2088 - loss: 10.1805 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 27/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2010 - loss: 9.9497 - val_accuracy: 0.2069 - val_loss: 10.2117\n",
            "Epoch 28/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2056 - loss: 9.1230 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 29/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1570 - loss: 9.6673 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 30/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1763 - loss: 9.7044 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 31/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1494 - loss: 9.2072 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 32/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1894 - loss: 10.6667 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 33/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2279 - loss: 9.5189 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 34/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1789 - loss: 9.9541 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 35/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1904 - loss: 11.1499 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 36/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1440 - loss: 9.7713 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 37/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1521 - loss: 9.8193 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 38/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2009 - loss: 8.7391 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 39/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1851 - loss: 10.1472 - val_accuracy: 0.2759 - val_loss: 10.2117\n",
            "Epoch 40/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.1846 - loss: 9.5852 - val_accuracy: 0.2069 - val_loss: 10.2117\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step\n",
            "logloss 1.7906042161992608\n",
            "\n",
            "Confusion Matrix:\n",
            " [[1 0 2 0 1 1]\n",
            " [0 0 0 0 0 2]\n",
            " [1 0 0 0 1 1]\n",
            " [1 0 2 0 2 2]\n",
            " [1 0 1 0 3 1]\n",
            " [3 0 0 0 1 2]]\n",
            "\n",
            "\n",
            "accuracy: 20.69 %\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "#setting up the ANN\n",
        "# first layer is number of features\n",
        "# last layer is output layer. Nodes=1 for binary classification\n",
        "model = Sequential()\n",
        "model.add(Dense(units=x_train.shape[1],activation='sigmoid'))\n",
        "model.add(Dense(units=6,activation='relu'))\n",
        "model.add(Dense(units=6,activation='softmax'))\n",
        "\n",
        "model.compile(loss='mse',optimizer='adam',metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x=x_train, y=y_train,epochs=40,validation_data=(x_test, y_test), batch_size = 10, verbose=1 )\n",
        "\n",
        "#making predictions\n",
        "y_pred_proba = model.predict(x_test)\n",
        "y_pred = np.argmax(y_pred_proba, axis=-1)\n",
        "\n",
        "from sklearn.metrics import log_loss\n",
        "print ('logloss',log_loss(y_test,y_pred_proba))\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print (\"\\nConfusion Matrix:\\n\",cm)\n",
        "print('\\n')\n",
        "print (\"accuracy: {0:5.2f} %\".format(accuracy_score(y_test, y_pred)*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a9928437-1871-4bf6-a95c-a5a5ee465840",
      "metadata": {
        "id": "a9928437-1871-4bf6-a95c-a5a5ee465840"
      },
      "outputs": [],
      "source": [
        "#hyperparameter tuning\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "\n",
        "def build_model(hp):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # Input + first hidden layer\n",
        "    model.add(Dense(\n",
        "        units=hp.Int('units_input', min_value=4, max_value=64, step=4),\n",
        "        activation='relu',\n",
        "        input_shape=(x_train.shape[1],)\n",
        "    ))\n",
        "\n",
        "    # Tune number of additional hidden layers (1 to 3)\n",
        "    for i in range(hp.Int('num_layers', 1, 3)):\n",
        "        model.add(Dense(\n",
        "            units=hp.Int(f'units_{i}', min_value=4, max_value=64, step=4),\n",
        "            activation='relu'\n",
        "        ))\n",
        "        #uncomment if you want a droput layer\n",
        "        # if hp.Boolean(f'dropout_{i}'):\n",
        "        #     model.add(Dropout(rate=hp.Float(f'dropout_rate_{i}', 0.1, 0.5, step=0.1)))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(units=6,activation='softmax'))\n",
        "\n",
        "    # Optimizer tuning\n",
        "    optimizer_choice = hp.Choice('optimizer', ['adam', 'rmsprop'])\n",
        "    if optimizer_choice == 'adam':\n",
        "        optimizer = Adam(learning_rate=hp.Float('adam_lr', 1e-4, 1e-2, sampling='log'))\n",
        "    else:\n",
        "        optimizer = RMSprop(learning_rate=hp.Float('rms_lr', 1e-4, 1e-2, sampling='log'))\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='mse', metrics=[\"accuracy\"])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "86f4829f-2c66-4801-995a-ed9323b0c322",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86f4829f-2c66-4801-995a-ed9323b0c322",
        "outputId": "6081682d-1c59-4227-9fb3-229fe412340b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9941e158-13c9-49ab-bf3d-64084b624511",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9941e158-13c9-49ab-bf3d-64084b624511",
        "outputId": "07c7f4b2-1c50-4f72-fa2f-37bce01f07a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 00m 05s]\n",
            "val_accuracy: 0.27586206793785095\n",
            "\n",
            "Best val_accuracy So Far: 0.3448275923728943\n",
            "Total elapsed time: 00h 00m 55s\n"
          ]
        }
      ],
      "source": [
        "#from kerastuner.tuners import RandomSearch\n",
        "import keras_tuner as kt\n",
        "\n",
        "tuner = kt.RandomSearch(build_model,\n",
        "                        objective='val_accuracy',       # minimize validation MAE\n",
        "                        max_trials=10,           # number of random configurations to test\n",
        "                        directory='tuner_directory',\n",
        "                        project_name='ann_random_search'\n",
        ")\n",
        "\n",
        "tuner.search(x_train, y_train, epochs=10, validation_data=(x_test, y_test), batch_size=5, verbose=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "792ce394-2e84-4e50-8b01-090ee33bcd8f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "792ce394-2e84-4e50-8b01-090ee33bcd8f",
        "outputId": "c157d372-b3d4-42fb-db80-282ebe73527d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters found:\n",
            "units_input: 36\n",
            "num_layers: 1\n",
            "units_0: 16\n",
            "optimizer: adam\n",
            "adam_lr: 0.00326110302723218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - accuracy: 0.1299 - loss: 10.9598 - val_accuracy: 0.2759 - val_loss: 10.2117\n",
            "Epoch 2/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1248 - loss: 10.5494 - val_accuracy: 0.0690 - val_loss: 10.2117\n",
            "Epoch 3/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0932 - loss: 10.1897 - val_accuracy: 0.2069 - val_loss: 10.2117\n",
            "Epoch 4/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2892 - loss: 10.4462 - val_accuracy: 0.1724 - val_loss: 10.2117\n",
            "Epoch 5/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1644 - loss: 10.0971 - val_accuracy: 0.3103 - val_loss: 10.2117\n",
            "Epoch 6/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1400 - loss: 10.6173 - val_accuracy: 0.2069 - val_loss: 10.2117\n",
            "Epoch 7/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2233 - loss: 9.0547 - val_accuracy: 0.1379 - val_loss: 10.2117\n",
            "Epoch 8/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1230 - loss: 11.0678 - val_accuracy: 0.2759 - val_loss: 10.2117\n",
            "Epoch 9/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1567 - loss: 11.1195 - val_accuracy: 0.2069 - val_loss: 10.2117\n",
            "Epoch 10/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1112 - loss: 9.3482 - val_accuracy: 0.2069 - val_loss: 10.2117\n",
            "Epoch 11/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2885 - loss: 10.1117 - val_accuracy: 0.0690 - val_loss: 10.2117\n",
            "Epoch 12/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0725 - loss: 10.3693 - val_accuracy: 0.2069 - val_loss: 10.2117\n",
            "Epoch 13/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2742 - loss: 8.6406 - val_accuracy: 0.1724 - val_loss: 10.2117\n",
            "Epoch 14/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1322 - loss: 9.5246 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 15/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2106 - loss: 9.8671 - val_accuracy: 0.1724 - val_loss: 10.2117\n",
            "Epoch 16/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0717 - loss: 10.7024 - val_accuracy: 0.2069 - val_loss: 10.2117\n",
            "Epoch 17/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3265 - loss: 11.0500 - val_accuracy: 0.1724 - val_loss: 10.2117\n",
            "Epoch 18/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0399 - loss: 10.7942 - val_accuracy: 0.2069 - val_loss: 10.2117\n",
            "Epoch 19/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.2556 - loss: 8.8062 - val_accuracy: 0.0690 - val_loss: 10.2117\n",
            "Epoch 20/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1082 - loss: 9.1916 - val_accuracy: 0.2069 - val_loss: 10.2117\n",
            "Epoch 21/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.2907 - loss: 10.8729 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 22/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2513 - loss: 9.5577 - val_accuracy: 0.1724 - val_loss: 10.2117\n",
            "Epoch 23/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1641 - loss: 9.1659 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 24/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1253 - loss: 10.0675 - val_accuracy: 0.1724 - val_loss: 10.2117\n",
            "Epoch 25/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1691 - loss: 9.8477 - val_accuracy: 0.2069 - val_loss: 10.2117\n",
            "Epoch 26/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2740 - loss: 10.0406 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 27/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1704 - loss: 9.7813 - val_accuracy: 0.0690 - val_loss: 10.2117\n",
            "Epoch 28/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1260 - loss: 10.1485 - val_accuracy: 0.2069 - val_loss: 10.2117\n",
            "Epoch 29/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2221 - loss: 9.9911 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 30/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1450 - loss: 9.9281 - val_accuracy: 0.0690 - val_loss: 10.2117\n",
            "Epoch 31/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1168 - loss: 9.3726 - val_accuracy: 0.2069 - val_loss: 10.2117\n",
            "Epoch 32/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1677 - loss: 10.2070 - val_accuracy: 0.2069 - val_loss: 10.2117\n",
            "Epoch 33/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2472 - loss: 9.6588 - val_accuracy: 0.0690 - val_loss: 10.2117\n",
            "Epoch 34/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2067 - loss: 9.3765 - val_accuracy: 0.0690 - val_loss: 10.2117\n",
            "Epoch 35/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1917 - loss: 9.9741 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 36/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0822 - loss: 9.2389 - val_accuracy: 0.2069 - val_loss: 10.2117\n",
            "Epoch 37/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1932 - loss: 10.4212 - val_accuracy: 0.0690 - val_loss: 10.2117\n",
            "Epoch 38/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1337 - loss: 10.0462 - val_accuracy: 0.2414 - val_loss: 10.2117\n",
            "Epoch 39/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1690 - loss: 9.2010 - val_accuracy: 0.2069 - val_loss: 10.2117\n",
            "Epoch 40/40\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2063 - loss: 9.7693 - val_accuracy: 0.1724 - val_loss: 10.2117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 8 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7a0e40fd94e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step\n",
            "\n",
            "Metrics for Test data after hyper paramter tuning\n",
            "logloss 1.7917594455917456\n",
            "\n",
            "Confusion Matrix:\n",
            " [[5 0 0 0 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [3 0 0 0 0 0]\n",
            " [7 0 0 0 0 0]\n",
            " [6 0 0 0 0 0]\n",
            " [6 0 0 0 0 0]]\n",
            "\n",
            "\n",
            "accuracy: 17.24 %\n"
          ]
        }
      ],
      "source": [
        "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
        "\n",
        "print(\"Best hyperparameters found:\")\n",
        "for k, v in best_hp.values.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "#extract best model\n",
        "best_model = tuner.get_best_models(1)[0]\n",
        "\n",
        "#refit\n",
        "best_model.fit(x=x_train, y=y_train,epochs=40,validation_data=(x_test, y_test), batch_size = 10, verbose=1 )\n",
        "\n",
        "y_pred_proba = best_model.predict(x_test)\n",
        "y_pred = np.argmax(y_pred_proba, axis=-1)\n",
        "\n",
        "print(\"\\nMetrics for Test data after hyper paramter tuning\")\n",
        "print ('logloss',log_loss(y_test,y_pred_proba))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print (\"\\nConfusion Matrix:\\n\",cm)\n",
        "print('\\n')\n",
        "print (\"accuracy: {0:5.2f} %\".format(accuracy_score(y_test, y_pred)*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure to summerize the results and highlight the best model and metrics."
      ],
      "metadata": {
        "id": "AqszBTRO7KQ7"
      },
      "id": "AqszBTRO7KQ7"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}